

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>rqa.modules.functional module &mdash; reasoning-qa 0.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="rqa.modules.initializer module" href="rqa.modules.initializer.html" />
    <link rel="prev" title="rqa.modules.activation module" href="rqa.modules.activation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contents/dataset_and_model.html">Dataset and Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="contents/experiments.html">Experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="contents/pretrained_vector.html">Pretrained Vector</a></li>
<li class="toctree-l1"><a class="reference internal" href="contents/tokens.html">Tokens</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="rqa.data.html">data</a></li>
<li class="toctree-l1"><a class="reference internal" href="rqa.learn.html">learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="rqa.metric.html">metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="rqa.model.html">model</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="rqa.modules.html">modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rqa.modules.html#subpackages">Subpackages</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="rqa.modules.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="rqa.modules.activation.html">rqa.modules.activation module</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">rqa.modules.functional module</a></li>
<li class="toctree-l3"><a class="reference internal" href="rqa.modules.initializer.html">rqa.modules.initializer module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="rqa.modules.html#module-rqa.modules">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rqa.tokens.html">tokens</a></li>
</ul>
<p class="caption"><span class="caption-text">Reports</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reports/historyqa.html">HistoryQA</a></li>
<li class="toctree-l1"><a class="reference internal" href="reports/korquad.html">KorQuAD</a></li>
<li class="toctree-l1"><a class="reference internal" href="reports/squad.html">SQuAD</a></li>
<li class="toctree-l1"><a class="reference internal" href="reports/wikisql.html">WikiSQL</a></li>
</ul>
<p class="caption"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">reasoning-qa</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="rqa.modules.html">rqa.modules package</a> &raquo;</li>
        
      <li>rqa.modules.functional module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rqa.modules.functional.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-rqa.modules.functional">
<span id="rqa-modules-functional-module"></span><h1>rqa.modules.functional module<a class="headerlink" href="#module-rqa.modules.functional" title="Permalink to this headline">¶</a></h1>
<p>some functional codes from allennlp: <a class="reference external" href="https://github.com/allenai/allennlp">https://github.com/allenai/allennlp</a></p>
<ul class="simple">
<li>add_masked_value : replace_masked_values (allennlp)</li>
<li>get_mask_from_tokens : get_mask_from_tokens (allennlp)</li>
<li>last_dim_masked_softmax : last_dim_masked_softmax (allennlp)</li>
<li>masked_softmax : masked_softmax (allennlp)</li>
<li>weighted_sum : weighted_sum (allennlp)</li>
</ul>
<dl class="function">
<dt id="rqa.modules.functional.add_masked_value">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">add_masked_value</code><span class="sig-paren">(</span><em>tensor</em>, <em>mask</em>, <em>value=-10000000.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#add_masked_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.add_masked_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Replaces all masked values in <code class="docutils literal notranslate"><span class="pre">tensor</span></code> with <code class="docutils literal notranslate"><span class="pre">replace_with</span></code>.  <code class="docutils literal notranslate"><span class="pre">mask</span></code> must be broadcastable
to the same shape as <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. We require that <code class="docutils literal notranslate"><span class="pre">tensor.dim()</span> <span class="pre">==</span> <span class="pre">mask.dim()</span></code>, as otherwise we
won’t know which dimensions of the mask to unsqueeze.</p>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.doc_product_similarity">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">doc_product_similarity</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em>, <em>scale_output=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#doc_product_similarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.doc_product_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>This similarity function simply computes the dot product between each pair of vectors, with an
optional scaling to reduce the variance of the output elements.</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>Args:</dt>
<dd>tensor1: tensor
tensor2: tensor</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Kwargs:</dt>
<dd><dl class="first last docutils">
<dt>scale_output <span class="classifier-delimiter">:</span> <span class="classifier">(bool), optional</span></dt>
<dd>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, we will scale the output by <code class="docutils literal notranslate"><span class="pre">math.sqrt(tensor.size(-1))</span></code>, to reduce the
variance in the result.</dd>
</dl>
</dd>
</dl>
</li>
</ul>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.expand">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">expand</code><span class="sig-paren">(</span><em>tensor</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#expand"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.get_mask_from_tokens">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">get_mask_from_tokens</code><span class="sig-paren">(</span><em>tokens</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#get_mask_from_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.get_mask_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the dictionary of tensors produced by a <code class="docutils literal notranslate"><span class="pre">Tokens</span></code> and returns a mask
with 0 where the tokens are padding, and 1 otherwise.</p>
<p>There could be several entries in the tensor dictionary with different shapes (e.g., one for
word ids, one for character ids).  In order to get a token mask, we use the tensor in
the dictionary with the lowest number of dimensions.  After subtracting <code class="docutils literal notranslate"><span class="pre">num_wrapping_dims</span></code>,
if this tensor has two dimensions we assume it has shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">num_tokens)</span></code>,
and use it for the mask.  If instead it has three dimensions, we assume it has shape
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">num_tokens,</span> <span class="pre">num_features)</span></code>, and sum over the last dimension to produce
the mask.  Most frequently this will be a character id tensor, but it could also be a
featurized representation of each token, etc.</p>
<p>NOTE: Our functions for generating masks create torch.LongTensors, because using
torch.ByteTensors  makes it easy to run into overflow errors
when doing mask manipulation, such as summing to get the lengths of sequences - see below.
&gt;&gt;&gt; mask = torch.ones([260]).byte()
&gt;&gt;&gt; mask.sum() # equals 260.
&gt;&gt;&gt; var_mask = torch.autograd.V(mask)
&gt;&gt;&gt; var_mask.sum() # equals 4, due to 8 bit precision - the sum overflows.</p>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.last_dim_masked_softmax">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">last_dim_masked_softmax</code><span class="sig-paren">(</span><em>x</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#last_dim_masked_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.last_dim_masked_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a tensor with 3 or more dimensions and does a masked softmax over the last dimension.  We
assume the tensor has shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">sequence_length)</span></code> and that the mask (if given)
has shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>.&lt;Paste&gt;
doc-string</p>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.masked_log_softmax">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">masked_log_softmax</code><span class="sig-paren">(</span><em>vector</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#masked_log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.masked_log_softmax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.masked_softmax">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">masked_softmax</code><span class="sig-paren">(</span><em>x</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#masked_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.masked_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.softmax(vector)</span></code> does not work if some elements of <code class="docutils literal notranslate"><span class="pre">vector</span></code> should be
masked.  This performs a softmax on just the non-masked portions of <code class="docutils literal notranslate"><span class="pre">vector</span></code>.  Passing
<code class="docutils literal notranslate"><span class="pre">None</span></code> in for the mask is also acceptable; you’ll just get a regular softmax.
We assume that both <code class="docutils literal notranslate"><span class="pre">vector</span></code> and <code class="docutils literal notranslate"><span class="pre">mask</span></code> (if given) have shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">vector_dim)</span></code>.
In the case that the input vector is completely masked, this function returns an array
of <code class="docutils literal notranslate"><span class="pre">0.0</span></code>. This behavior may cause <code class="docutils literal notranslate"><span class="pre">NaN</span></code> if this is used as the last layer of a model
that uses categorical cross-entropy loss.</p>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.masking">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">masking</code><span class="sig-paren">(</span><em>tensor</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#masking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.masking" title="Permalink to this definition">¶</a></dt>
<dd><p>Tensor masking operation</p>
</dd></dl>

<dl class="function">
<dt id="rqa.modules.functional.weighted_sum">
<code class="descclassname">rqa.modules.functional.</code><code class="descname">weighted_sum</code><span class="sig-paren">(</span><em>attention</em>, <em>matrix</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rqa/modules/functional.html#weighted_sum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rqa.modules.functional.weighted_sum" title="Permalink to this definition">¶</a></dt>
<dd><p>if attention a matrix of vectors and a set of weights over the rows in the matrix (which we call an
“attention” vector), and returns a weighted sum of the rows in the matrix.  This is the typical
computation performed after an attention mechanism.
Note that while we call this a “matrix” of vectors and an attention “vector”, we also handle
higher-order tensors.  We always sum over the second-to-last dimension of the “matrix”, and we
assume that all dimensions in the “matrix” prior to the last dimension are matched in the
“vector”.  Non-matched dimensions in the “vector” must be <cite>directly after the batch dimension</cite>.
For example, say I have a “matrix” with dimensions <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">num_words,</span>
<span class="pre">embedding_dim)</span></code>.  The attention “vector” then must have at least those dimensions, and could
have more. Both:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">num_words)</span></code> (distribution over words for each query)</li>
<li><code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_documents,</span> <span class="pre">num_queries,</span> <span class="pre">num_words)</span></code> (distribution over words in a
query for each document)</li>
</ul>
</div></blockquote>
<p>are valid input “vectors”, producing tensors of shape:
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">embedding_dim)</span></code> and
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_documents,</span> <span class="pre">num_queries,</span> <span class="pre">embedding_dim)</span></code> respectively.</p>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rqa.modules.initializer.html" class="btn btn-neutral float-right" title="rqa.modules.initializer module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rqa.modules.activation.html" class="btn btn-neutral" title="rqa.modules.activation module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Dongjun Lee

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>